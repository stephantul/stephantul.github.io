---
layout: post
title:  "Sparse retrieval"
date:   2025-03-28-00:00:00 +0000
categories: sparse retrieval
---

I've been diving deeply into sparse retrieval recently. It's pretty interesting, so I'm going to write up some of my notes here in the hope that they will be useful to someone else. 

# What is retrieval?

Retrieval is the task of getting the relevant documents from some collection given some user query. Good retrieval systems retrieve rank relevant documents more highly than irrelevant documents. The most well-known retrieval system in the world is probably Google or the prototypical "search engine" your company uses internally.

What is particularly interesting about retrieval is that user queries and the documents that answer those queries usually don't have that many words in common: the similarity between queries and documents tends to be really low. Let's call this "the mismatch problem". Because of this problem, you can't necessarily rely on similarity measures to find relevant documents, unless you explicitly train a similarity function to do this for you.

# What is sparse retrieval?

Now that you know what retrieval is, let's think about sparse retrieval.

The best way to describe sparse retrieval is by contrasting it with dense retrieval, simply because dense retrieval is probably familiar to you in the context of RAG. As you probably know, dense models produce relatively low-dimensional embeddings, with very few or no zero coefficients (hence "dense"). Dense models don't necessarily suffer from the mismatch problem I introduced just now, since dense embeddings have graded similarity; a sentence such as "I love cats" will be more similar to "I love dogs" than to "I love robots", because cats are more similar to dogs than to robots. In addition to this, dense retrieval tends to work well, because dense models can be trained to ensure that queries and the documents that answer them are actually very close in embedding space.

Cue sparse retrieval. In contrast with dense models, which produce e.g., 768-dimensional vectors, sparse models produce much higher-dimensional vectors, e.g., 100000 dimensional, with the catch that only very few of these dimensions are actually non-zero. In regular sparse retrieval, the basis of the vector space in which you embed documents is lexical: each of the 100000 dimensions in your vector corresponds to a specific word or lexical item. If a word occurs in a document or query, the coefficient belonging to that specific word becomes non-zero. The coefficients in sparse systems are _higher_ for important words, and usually very low for stopwords, like "the" or "a".

Note that there are many systems that produce sparse vectors that are not based on words. In what follows, I will not consider these, because they are actually hybrid systems: they typically sparsify dense embeddings (e.g., see [this paper](https://arxiv.org/pdf/2503.01776)), and don't really suffer from the mismatch problem as much.

Let's return to the example from before: unlike a dense system, a lexical sparse retrieval system will assign the same similarity to "I love dogs", "I love cats" and "I love robots", since all sentence share two words. This is regardless of the scores you assign to the specific words.

If this seems like a big disadvantage to you, you'd be right! So, before getting to that, I'll lead with some reasons why sparse retrieval is nice:

1. because you can use an [inverted index](https://en.wikipedia.org/wiki/Inverted_index) to store documents, using and implementing a sparse model is easier than a dense model, for which you probably need a vector database.
2. retrieval is interpretable: it is very easy to see which documents get selected and why; documents without any shared vocab can never get selected.
3. simple sparse retrieval systems can outperform dense retrieval models: an untuned [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) model scores 0.58 NDCG@10 on [Nanobeir](https://huggingface.co/collections/zeta-alpha-ai/nanobeir-66e1a0af21dfd93e620cd9f6), which is better than [minilm](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2), and almost matches [bge-m3](https://huggingface.co/BAAI/bge-m3). Note that beyond BM25, there is no real speed advantage to using sparse models, since newere sparse models also use transformer-based encoders.
4. sparse retrieval models can be updated in flight: if a new term enters the vocabulary, there is no need to update the entire index.

One thing that did surprise me when I was writing this, is that the _retrieval_ part of sparse retrieval is not actually faster than HNSW-based dense retrieval systems. I always assumed that this was the case. The reason for this is that, for large corpora, the inverted index becomes inefficient again. For example, if your corpus contains 1 billion documents, almost every word you can think of will occur thousands if not tens of thousands of times.

# What does a good sparse retrieval model do?

So, if you've been keeping track of what a sparse retrieval system looks like, you might have realized that there's only four things you can actually optimize: 

*1. the scores in the index:* if you can assign better coefficients, documents will be more easily retrievable.
*2. the scores of the query:* if the query assigns better scores, you can retrieve better documents.
*3: the terms in the documents:* if you add additional informative terms to the document, you increase coverage.
*4. the terms in the query:* if you add additional informative terms to the query, you can find better documents.

Note that optimizations 1, 2, and 4 do not impact index size at all, while the third should be exercised carefully, since your index, and hence your retrieval speed, is directly impacted by lower sparsity. Good retrieval models tend to focus on a combination of 